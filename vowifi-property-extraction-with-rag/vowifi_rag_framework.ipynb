{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "# %pip install numpy pandas tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing\n",
    "# %pip install PyMuPDF pdfplumber python-docx\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components\n",
    "# %pip install langchain langchain-core langchain-community faiss-cpu\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers and model handling\n",
    "# %pip install transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aa5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for the VoWiFi RAG Pipeline\"\"\"\n",
    "    # Model configuration\n",
    "    model_name: str = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    embedding_model: str = \"BAAI/bge-base-en-v1.5\"\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    quantization: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "    \n",
    "    # Text processing\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "    max_new_tokens: int = 400\n",
    "    temperature: float = 0.2\n",
    "    \n",
    "    # Retrieval settings\n",
    "    retriever_k: int = 4\n",
    "    similarity_threshold: float = 0.7\n",
    "    \n",
    "    # Property extraction\n",
    "    min_property_length: int = 20\n",
    "    property_patterns: List[str] = None\n",
    "    \n",
    "    # Performance optimization settings\n",
    "    batch_size: int = 32\n",
    "    max_workers: int = 4\n",
    "    use_parallel_processing: bool = True\n",
    "    lazy_loading: bool = True\n",
    "    memory_optimization: bool = True\n",
    "    cache_embeddings: bool = True\n",
    "    \n",
    "    # File paths\n",
    "    documents_dir: str = \"rfcs\"\n",
    "    vector_store_path: str = \"vowifi_vectorstore\"\n",
    "    output_dir: str = \"extracted_properties\"\n",
    "    cache_dir: str = \".cache\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.property_patterns is None:\n",
    "            self.property_patterns = [\n",
    "                \"shall\", \"must\", \"MUST\", \"if\", \"IF\",\n",
    "                \"includes\", \"sends\", \"contains\", \"provided\",\n",
    "                \"acceptable\", \"unable\", \"required\", \"proposed\",\n",
    "                \"authentication\", \"authorization\", \"security\"\n",
    "            ]\n",
    "        \n",
    "        # Optimize batch size based on available memory\n",
    "        if self.memory_optimization:\n",
    "            self.batch_size = min(self.batch_size, 16)  # Conservative for GPU memory\n",
    "            \n",
    "        # Adjust workers based on CPU count\n",
    "        if self.use_parallel_processing:\n",
    "            import multiprocessing\n",
    "            cpu_count = multiprocessing.cpu_count()\n",
    "            self.max_workers = min(self.max_workers, cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Advanced document processing for VoWiFi specifications\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self._cache_dir = Path(config.cache_dir) / \"documents\"\n",
    "        self._cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def _get_cache_path(self, file_path: str) -> Path:\n",
    "        \"\"\"Generate cache path for processed document\"\"\"\n",
    "        file_hash = hashlib.md5(str(file_path).encode()).hexdigest()\n",
    "        return self._cache_dir / f\"{file_hash}.pkl\"\n",
    "    \n",
    "    def _load_from_cache(self, file_path: str) -> Optional[str]:\n",
    "        \"\"\"Load processed text from cache if available and valid\"\"\"\n",
    "        cache_path = self._get_cache_path(file_path)\n",
    "        if not cache_path.exists():\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            file_stat = os.stat(file_path)\n",
    "            cache_stat = os.stat(cache_path)\n",
    "            \n",
    "            # Check if cache is newer than the file\n",
    "            if cache_stat.st_mtime >= file_stat.st_mtime:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load cache for {file_path}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, file_path: str, content: str):\n",
    "        \"\"\"Save processed text to cache\"\"\"\n",
    "        try:\n",
    "            cache_path = self._get_cache_path(file_path)\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(content, f)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to save cache for {file_path}: {e}\")\n",
    "    \n",
    "    def extract_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF with multiple fallback strategies\"\"\"\n",
    "        # Check cache first\n",
    "        if self.config.lazy_loading:\n",
    "            cached_content = self._load_from_cache(pdf_path)\n",
    "            if cached_content is not None:\n",
    "                return cached_content\n",
    "        \n",
    "        text = \"\"\n",
    "        \n",
    "        # Strategy 1: Try PyMuPDF first (best for general text)\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "            if text.strip():\n",
    "                self._save_to_cache(pdf_path, text)\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"PyMuPDF failed for {pdf_path}: {e}\")\n",
    "        \n",
    "        # Strategy 2: Try pdfplumber (better for structured content)\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "            if text.strip():\n",
    "                self._save_to_cache(pdf_path, text)\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"pdfplumber failed for {pdf_path}: {e}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_from_docx(self, docx_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        # Check cache first\n",
    "        if self.config.lazy_loading:\n",
    "            cached_content = self._load_from_cache(docx_path)\n",
    "            if cached_content is not None:\n",
    "                return cached_content\n",
    "        \n",
    "        try:\n",
    "            doc = Document(docx_path)\n",
    "            content = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "            self._save_to_cache(docx_path, content)\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting from DOCX {docx_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_from_txt(self, txt_path: str) -> str:\n",
    "        \"\"\"Extract text from TXT file\"\"\"\n",
    "        # Check cache first\n",
    "        if self.config.lazy_loading:\n",
    "            cached_content = self._load_from_cache(txt_path)\n",
    "            if cached_content is not None:\n",
    "                return cached_content\n",
    "        \n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            self._save_to_cache(txt_path, content)\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting from TXT {txt_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _process_single_document(self, file_path: str) -> Optional[LangchainDocument]:\n",
    "        \"\"\"Process a single document and return LangChain Document\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            content = self.extract_from_pdf(str(file_path))\n",
    "        elif file_path.suffix.lower() == '.docx':\n",
    "            content = self.extract_from_docx(str(file_path))\n",
    "        elif file_path.suffix.lower() == '.txt':\n",
    "            content = self.extract_from_txt(str(file_path))\n",
    "        else:\n",
    "            self.logger.warning(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        if not content.strip():\n",
    "            self.logger.warning(f\"No content extracted from {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": str(file_path),\n",
    "            \"filename\": file_path.name,\n",
    "            \"file_type\": file_path.suffix.lower(),\n",
    "            \"char_count\": len(content)\n",
    "        }\n",
    "        \n",
    "        return LangchainDocument(page_content=content, metadata=metadata)\n",
    "    \n",
    "    def process_document(self, file_path: str) -> LangchainDocument:\n",
    "        \"\"\"Process a single document and return LangChain Document (legacy method)\"\"\"\n",
    "        return self._process_single_document(file_path)\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> List[LangchainDocument]:\n",
    "        \"\"\"Process all documents in a directory with parallel processing\"\"\"\n",
    "        documents = []\n",
    "        directory_path = Path(directory_path)\n",
    "        \n",
    "        if not directory_path.exists():\n",
    "            self.logger.error(f\"Directory does not exist: {directory_path}\")\n",
    "            return documents\n",
    "        \n",
    "        supported_extensions = {'.pdf', '.docx', '.txt'}\n",
    "        files = [f for f in directory_path.rglob('*') \n",
    "                if f.is_file() and f.suffix.lower() in supported_extensions]\n",
    "        \n",
    "        if not files:\n",
    "            self.logger.warning(f\"No supported files found in {directory_path}\")\n",
    "            return documents\n",
    "        \n",
    "        # Process documents in parallel if enabled\n",
    "        if self.config.use_parallel_processing and len(files) > 1:\n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                future_to_file = {\n",
    "                    executor.submit(self._process_single_document, str(file_path)): file_path \n",
    "                    for file_path in files\n",
    "                }\n",
    "                \n",
    "                for future in tqdm(as_completed(future_to_file), \n",
    "                                 total=len(files), desc=\"Processing documents\"):\n",
    "                    file_path = future_to_file[future]\n",
    "                    try:\n",
    "                        doc = future.result()\n",
    "                        if doc:\n",
    "                            documents.append(doc)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        else:\n",
    "            # Sequential processing for small file counts or when parallel processing is disabled\n",
    "            for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "                doc = self._process_single_document(str(file_path))\n",
    "                if doc:\n",
    "                    documents.append(doc)\n",
    "        \n",
    "        self.logger.info(f\"Processed {len(documents)} documents from {directory_path}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoWiFiPropertyExtractor:\n",
    "    \"\"\"Specialized property extraction for VoWiFi specifications\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Enhanced property patterns for VoWiFi domain\n",
    "        self.property_indicators = {\n",
    "            'requirements': ['shall', 'must', 'MUST', 'required', 'mandatory'],\n",
    "            'conditions': ['if', 'IF', 'when', 'WHEN', 'unless', 'provided that'],\n",
    "            'behaviors': ['sends', 'transmits', 'receives', 'processes', 'includes'],\n",
    "            'constraints': ['unable', 'cannot', 'forbidden', 'not allowed'],\n",
    "            'specifications': ['contains', 'comprises', 'consists of', 'defined as'],\n",
    "            'security': ['authentication', 'authorization', 'encrypt', 'decrypt', 'certificate']\n",
    "        }\n",
    "        \n",
    "        # Cache for property checks\n",
    "        self._property_check_cache = {}\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def is_potential_property(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains property indicators (cached for performance)\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for property patterns\n",
    "        for category, patterns in self.property_indicators.items():\n",
    "            if any(pattern.lower() in text_lower for pattern in patterns):\n",
    "                return True\n",
    "        \n",
    "        # Check for specific VoWiFi terms\n",
    "        vowifi_terms = ['ue', 'epdg', 'ike', 'ipsec', 'esp', 'pdn', 'aaa', 'msk']\n",
    "        if any(term in text_lower for term in vowifi_terms):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extract_properties_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract properties from raw text with optimized processing\"\"\"\n",
    "        properties = []\n",
    "        \n",
    "        # Use more efficient text splitting\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        \n",
    "        # Use batch processing for large texts\n",
    "        if len(sentences) > self.config.batch_size:\n",
    "            for i in range(0, len(sentences), self.config.batch_size):\n",
    "                batch = sentences[i:i + self.config.batch_size]\n",
    "                batch_properties = self._process_sentence_batch(batch)\n",
    "                properties.extend(batch_properties)\n",
    "        else:\n",
    "            properties = self._process_sentence_batch(sentences)\n",
    "        \n",
    "        return properties\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Efficiently split text into sentences\"\"\"\n",
    "        # Replace newlines with spaces for better sentence detection\n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # Split on common sentence endings\n",
    "        sentences = []\n",
    "        current = \"\"\n",
    "        \n",
    "        for char in text:\n",
    "            current += char\n",
    "            if char in '.!?':\n",
    "                sentence = current.strip()\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                current = \"\"\n",
    "        \n",
    "        # Add remaining text if any\n",
    "        if current.strip():\n",
    "            sentences.append(current.strip())\n",
    "            \n",
    "        return sentences\n",
    "    \n",
    "    def _process_sentence_batch(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"Process a batch of sentences for property extraction\"\"\"\n",
    "        properties = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Filter by length first (fastest check)\n",
    "            if len(sentence) < self.config.min_property_length:\n",
    "                continue\n",
    "                \n",
    "            # Check if it's a potential property\n",
    "            if self.is_potential_property(sentence):\n",
    "                # Clean and normalize the sentence\n",
    "                cleaned = self._clean_property_text(sentence)\n",
    "                if cleaned:\n",
    "                    properties.append(cleaned)\n",
    "        \n",
    "        return properties\n",
    "    \n",
    "    def _clean_property_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize property text with better performance\"\"\"\n",
    "        # Remove extra whitespace (faster than multiple split/join)\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove common artifacts\n",
    "        text = text.replace('\\x00', '').replace('\\ufffd', '')\n",
    "        \n",
    "        # Ensure proper sentence ending\n",
    "        if not text.endswith('.'):\n",
    "            text += '.'\n",
    "        \n",
    "        return text if len(text) > self.config.min_property_length else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoWiFiRAGPipeline:\n",
    "    \"\"\"Main RAG Pipeline for VoWiFi Property Extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.logger = self._setup_logging()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.document_processor = DocumentProcessor(config)\n",
    "        self.property_extractor = VoWiFiPropertyExtractor(config)\n",
    "        \n",
    "        # RAG components (initialized later)\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.llm = None\n",
    "        self.rag_chain = None\n",
    "        self.text_splitter = None\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('vowifi_rag.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    def _monitor_memory(self):\n",
    "        \"\"\"Monitor memory usage and perform cleanup if needed\"\"\"\n",
    "        if self.config.memory_optimization:\n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Clear CUDA cache if using GPU\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize embedding and language models with optimization\"\"\"\n",
    "        self.logger.info(\"Initializing models...\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize embeddings with caching\n",
    "            cache_path = Path(self.config.cache_dir) / \"embeddings\"\n",
    "            cache_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=self.config.embedding_model,\n",
    "                cache_folder=str(cache_path)\n",
    "            )\n",
    "            \n",
    "            # Initialize text splitter\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunk_size,\n",
    "                chunk_overlap=self.config.chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Initialize LLM\n",
    "            self._initialize_llm()\n",
    "            \n",
    "            self.logger.info(\"Models initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize models: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Monitor memory after initialization\n",
    "        self._monitor_memory()\n",
    "    \n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"Initialize the language model with quantization if specified\"\"\"\n",
    "        model_kwargs = {}\n",
    "        \n",
    "        if self.config.quantization:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=getattr(torch, self.config.torch_dtype)\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = quantization_config\n",
    "        \n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                device_map=self.config.device_map,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Create pipeline\n",
    "            text_generation_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                temperature=self.config.temperature,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.1,\n",
    "                return_full_text=False,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "            )\n",
    "            \n",
    "            self.llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize LLM: {e}\")\n",
    "            # Fallback to a lighter model or disable LLM functionality\n",
    "            self.llm = None\n",
    "    \n",
    "    def build_knowledge_base(self, documents_dir: str = None) -> int:\n",
    "        \"\"\"Build the vector knowledge base from documents with optimization\"\"\"\n",
    "        if documents_dir is None:\n",
    "            documents_dir = self.config.documents_dir\n",
    "        \n",
    "        self.logger.info(f\"Building knowledge base from {documents_dir}\")\n",
    "        \n",
    "        try:\n",
    "            # Process documents\n",
    "            documents = self.document_processor.process_directory(documents_dir)\n",
    "            if not documents:\n",
    "                self.logger.error(\"No documents processed\")\n",
    "                return 0\n",
    "            \n",
    "            # Split documents into chunks with progress tracking\n",
    "            self.logger.info(\"Splitting documents into chunks...\")\n",
    "            chunked_docs = []\n",
    "            \n",
    "            for doc in tqdm(documents, desc=\"Chunking documents\"):\n",
    "                chunks = self.text_splitter.split_documents([doc])\n",
    "                chunked_docs.extend(chunks)\n",
    "                \n",
    "                # Memory optimization: process in batches\n",
    "                if self.config.memory_optimization and len(chunked_docs) > 1000:\n",
    "                    self._monitor_memory()\n",
    "            \n",
    "            self.logger.info(f\"Created {len(chunked_docs)} chunks from {len(documents)} documents\")\n",
    "            \n",
    "            # Create vector store with batch processing\n",
    "            self.logger.info(\"Creating vector embeddings...\")\n",
    "            self.vectorstore = FAISS.from_documents(chunked_docs, self.embeddings)\n",
    "            \n",
    "            # Save vector store\n",
    "            self.vectorstore.save_local(self.config.vector_store_path)\n",
    "            self.logger.info(f\"Vector store saved to {self.config.vector_store_path}\")\n",
    "            \n",
    "            self.logger.info(\"Knowledge base built successfully\")\n",
    "            \n",
    "            return len(chunked_docs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to build knowledge base: {e}\")\n",
    "            return 0\n",
    "        finally:\n",
    "            self._monitor_memory()\n",
    "    \n",
    "    def load_knowledge_base(self) -> bool:\n",
    "        \"\"\"Load existing vector knowledge base with error handling\"\"\"\n",
    "        try:\n",
    "            if self.embeddings is None:\n",
    "                self.initialize_models()\n",
    "            \n",
    "            # Check if vector store exists\n",
    "            vector_store_path = Path(self.config.vector_store_path)\n",
    "            if not vector_store_path.exists():\n",
    "                self.logger.info(\"Vector store does not exist\")\n",
    "                return False\n",
    "            \n",
    "            self.vectorstore = FAISS.load_local(\n",
    "                self.config.vector_store_path, \n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            self.logger.info(f\"Vector store loaded from {self.config.vector_store_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_rag_chain(self):\n",
    "        \"\"\"Setup the RAG chain for question answering\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            self.logger.error(\"Vector store not initialized. Call build_knowledge_base() first.\")\n",
    "            return\n",
    "        \n",
    "        # Create retriever\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={'k': self.config.retriever_k}\n",
    "        )\n",
    "        \n",
    "        # Create prompt template for VoWiFi property extraction\n",
    "        prompt_template =   \"\"\"<|system|>  \n",
    "                            % Role Declaration  \n",
    "                            You are a Vo-WiFi expert. Your task is to extract properties of Vo-WiFi from the contexts given from specifications.  \n",
    "\n",
    "                            Below you will find the basic structure of Properties in a Vo-WiFi specification.  \n",
    "\n",
    "                            % Rules  \n",
    "                            Properties often  \n",
    "                            1. Use \"shall\", \"must\", or \"should\" to indicate mandatory actions or strong recommendations.  \n",
    "                            2. Describe specific actions, such as taking input, sending parameters, or generating output.  \n",
    "                            3. Include terms like Input, Output, Parameter etc.  \n",
    "                            4. Specify interactions between network components (e.g., UE, ePDG, AAA Server) and the data exchanged (e.g., AUTH parameter, Notify payload).  \n",
    "                            5. Describe steps in a process and the dependencies between them, specify conditions or contexts for actions to occur.  \n",
    "\n",
    "                            % Instructions  \n",
    "                            1. Be concise while generating; only give the extracted properties as a response, and don't add anything on your own.  \n",
    "\n",
    "                            % Example Properties  \n",
    "                            Some example properties are:  \n",
    "                            1. The UE shall take its own copy of the MSK as input to generate the AUTH parameter to authenticate the first IKE_SA_INIT message.  \n",
    "                            2. The AUTH parameter is sent to the ePDG. The UE includes a Notify payload ANOTHER_AUTH_FOLLOWS indicating to the ePDG that another authentication and authorization round will follow.  \n",
    "                            3. The UE sends the identity in the private network in IDi payload that is used for the next authentication and authorization with the External AAA Server and without an AUTH payload.  \n",
    "\n",
    "                            % Context Block  \n",
    "                            You will find the required information about vo-wifi properties in the following context:  \n",
    "\n",
    "                            {context} % retrieved from the specifications  \n",
    "\n",
    "                            % Assistant Output Section\n",
    "                            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            template=prompt_template,\n",
    "        )\n",
    "        \n",
    "        # Create RAG chain\n",
    "        self.rag_chain = (\n",
    "            {\"context\": retriever}\n",
    "            | prompt \n",
    "            | self.llm \n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\"RAG chain setup completed\")\n",
    "    \n",
    "    def extract_properties_from_documents(self, output_file: str = None) -> List[str]:\n",
    "        \"\"\"Extract all properties from the knowledge base\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            self.logger.error(\"Vector store not initialized\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(\"Extracting properties from all documents...\")\n",
    "        \n",
    "        # Get all documents from vector store\n",
    "        all_docs = self.vectorstore.docstore._dict.values()\n",
    "        all_properties = []\n",
    "        \n",
    "        # Use batch processing for large document sets\n",
    "        docs_list = list(all_docs)\n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        for i in tqdm(range(0, len(docs_list), batch_size), desc=\"Processing document batches\"):\n",
    "            batch_docs = docs_list[i:i + batch_size]\n",
    "            batch_properties = []\n",
    "            \n",
    "            for doc in batch_docs:\n",
    "                properties = self.property_extractor.extract_properties_from_text(doc.page_content)\n",
    "                batch_properties.extend(properties)\n",
    "            \n",
    "            # Remove duplicates within batch\n",
    "            for prop in batch_properties:\n",
    "                if prop not in all_properties:\n",
    "                    all_properties.append(prop)\n",
    "            \n",
    "            # Memory optimization\n",
    "            if self.config.memory_optimization and i % (batch_size * 10) == 0:\n",
    "                self._monitor_memory()\n",
    "        \n",
    "        # Save properties\n",
    "        if output_file is None:\n",
    "            output_file = os.path.join(self.config.output_dir, \"vowifi_properties.txt\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for i, prop in enumerate(all_properties, 1):\n",
    "                f.write(f\"{i}. {prop}\\n\")\n",
    "        \n",
    "        self.logger.info(f\"Extracted {len(all_properties)} properties to {output_file}\")\n",
    "        return all_properties\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        if self.rag_chain is None:\n",
    "            self.setup_rag_chain()\n",
    "        \n",
    "        try:\n",
    "            response = self.rag_chain.invoke(question)\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Query failed: {e}\")\n",
    "            return f\"Error processing query: {e}\"\n",
    "    \n",
    "    def extract_properties_for_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract properties relevant to a specific query\"\"\"\n",
    "        # First get relevant context\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={'k': self.config.retriever_k * 2}  # Get more context\n",
    "        )\n",
    "        \n",
    "        relevant_docs = retriever.get_relevant_documents(query)\n",
    "        combined_context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Extract properties from relevant context\n",
    "        properties = self.property_extractor.extract_properties_from_text(combined_context)\n",
    "        return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1732c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to extract VoWiFi properties from RFC documents\"\"\"\n",
    "    print(\"VoWiFi Property Extraction Framework\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Use default configuration\n",
    "        config = RAGConfig()\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        pipeline = VoWiFiRAGPipeline(config)\n",
    "        \n",
    "        # Initialize models\n",
    "        print(\"Initializing models...\")\n",
    "        pipeline.initialize_models()\n",
    "        print(\"Models initialized successfully\")\n",
    "        \n",
    "        # Build knowledge base if it doesn't exist\n",
    "        if not pipeline.load_knowledge_base():\n",
    "            print(\"Building knowledge base from RFC documents...\")\n",
    "            chunks_created = pipeline.build_knowledge_base()\n",
    "            print(f\"Knowledge base built with {chunks_created} chunks\")\n",
    "        else:\n",
    "            print(\"Loaded existing knowledge base\")\n",
    "        \n",
    "        # Extract properties\n",
    "        print(\"Extracting VoWiFi properties from documents...\")\n",
    "        properties = pipeline.extract_properties_from_documents()\n",
    "        \n",
    "        # Simple output\n",
    "        print(f\"Extracted {len(properties)} properties\")\n",
    "        print(f\"Properties saved to: {os.path.join(config.output_dir, 'vowifi_properties.txt')}\")\n",
    "        \n",
    "        return len(properties)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        logging.error(f\"Main execution failed: {e}\", exc_info=True)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bitsandbytes\n",
    "result = main()\n",
    "if result > 0:\n",
    "    print(f\"Successfully extracted {result} properties!\")\n",
    "else:\n",
    "    print(\"Extraction failed. Check the logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760b0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
